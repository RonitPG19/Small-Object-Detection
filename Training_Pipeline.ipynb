{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsMH3had0f3J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# STEP 1: Mount Google Drive for Persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory structure\n",
        "import os\n",
        "project_root = '/content/drive/MyDrive/satellite_detection'\n",
        "dirs = ['checkpoints', 'results', 'datasets', 'predictions', 'backups']\n",
        "for d in dirs:\n",
        "    os.makedirs(os.path.join(project_root, d), exist_ok=True)\n",
        "\n",
        "print(\"Google Drive mounted and directories created\")\n",
        "\n",
        "\n",
        "# STEP 2: Install Dependencies\n",
        "\n",
        "!pip install -q ultralytics opencv-python-headless matplotlib tensorboard\n",
        "\n",
        "# Verify installation\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "\n",
        "# STEP 3: Check GPU Availability\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Setup for DOTA\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# For quick testing: DOTA8 (8-image subset)\n",
        "# For full training: DOTAv1, DOTAv2, or xView\n",
        "\n",
        "# DOTA classes (15 categories)\n",
        "DOTA_CLASSES = [\n",
        "    'plane', 'ship', 'storage-tank', 'baseball-diamond',\n",
        "    'tennis-court', 'basketball-court', 'ground-track-field',\n",
        "    'harbor', 'bridge', 'large-vehicle', 'small-vehicle',\n",
        "    'helicopter', 'roundabout', 'soccer-ball-field', 'swimming-pool'\n",
        "]\n",
        "\n",
        "print(f\"Dataset classes ({len(DOTA_CLASSES)}): {DOTA_CLASSES}\")\n",
        "\n",
        "# Dataset will auto-download on first training run\n",
        "# For custom dataset, follow this structure:\n",
        "\"\"\"\n",
        "datasets/DOTA/\n",
        "  ├── images/\n",
        "  │   ├── train/\n",
        "  │   ├── val/\n",
        "  │   └── test/\n",
        "  └── labels/\n",
        "      ├── train/\n",
        "      ├── val/\n",
        "      └── test/\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Uav78sPN_awH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Advanced Training Pipeline with Auto-Resume\n",
        "\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import shutil\n",
        "\n",
        "class SatelliteObjectDetector:\n",
        "    \"\"\"\n",
        "    Complete training pipeline for small object detection in satellite imagery\n",
        "    Optimized for Google Colab T4 GPU with automatic checkpointing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model_size='n',  # n, s, m, l, x\n",
        "                 data_yaml='DOTAv1.yaml',\n",
        "                 project_root='/content/drive/MyDrive/satellite_detection'):\n",
        "        \"\"\"\n",
        "        Initialize detector\n",
        "\n",
        "        Args:\n",
        "            model_size: YOLOv11 model size (n=nano, s=small, m=medium, l=large, x=xlarge)\n",
        "            data_yaml: Dataset configuration (dota8.yaml, DOTAv1.yaml, or custom)\n",
        "            project_root: Root directory for all outputs\n",
        "        \"\"\"\n",
        "        self.model_size = model_size\n",
        "        self.data_yaml = data_yaml\n",
        "        self.project_root = project_root\n",
        "        self.checkpoint_dir = os.path.join(project_root, 'checkpoints')\n",
        "        self.results_dir = os.path.join(project_root, 'results')\n",
        "        self.backup_dir = os.path.join(project_root, 'backups')\n",
        "\n",
        "        # Ensure directories exist\n",
        "        for d in [self.checkpoint_dir, self.results_dir, self.backup_dir]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "        self.model = None\n",
        "        self.training_history = []\n",
        "\n",
        "    def find_latest_checkpoint(self):\n",
        "        \"\"\"Find most recent checkpoint to resume training\"\"\"\n",
        "        # Check Ultralytics default location\n",
        "        last_pt = os.path.join(self.results_dir, 'train/weights/last.pt')\n",
        "        if os.path.exists(last_pt):\n",
        "            print(f\"Found checkpoint: {last_pt}\")\n",
        "            return last_pt\n",
        "\n",
        "        # Check custom checkpoint directory\n",
        "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.endswith('.pt')]\n",
        "        if checkpoints:\n",
        "            latest = max([os.path.join(self.checkpoint_dir, f) for f in checkpoints],\n",
        "                        key=os.path.getmtime)\n",
        "            print(f\"Found checkpoint: {latest}\")\n",
        "            return latest\n",
        "\n",
        "        print(\"No checkpoint found, starting fresh\")\n",
        "        return None\n",
        "\n",
        "    def backup_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"Create timestamped backup of checkpoint\"\"\"\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            return\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        backup_name = f\"backup_{self.model_size}_{timestamp}.pt\"\n",
        "        backup_path = os.path.join(self.backup_dir, backup_name)\n",
        "        shutil.copy2(checkpoint_path, backup_path)\n",
        "        print(f\"Backup created: {backup_name}\")\n",
        "\n",
        "    def train(self,\n",
        "              epochs=100,\n",
        "              imgsz=1024,  # DOTA standard size\n",
        "              batch=8,     # Adjust for T4 GPU (16GB)\n",
        "              resume=True,\n",
        "              patience=50,\n",
        "              save_period=10,\n",
        "              lr0=0.01,\n",
        "              optimizer='Adam',\n",
        "              augment=True,\n",
        "              device=0,\n",
        "              **kwargs):\n",
        "        \"\"\"\n",
        "        Train YOLOv11-OBB model with automatic checkpointing\n",
        "\n",
        "        Args:\n",
        "            epochs: Total training epochs\n",
        "            imgsz: Input image size (1024 for DOTA, reduce to 640 if OOM)\n",
        "            batch: Batch size (reduce to 4 if out of memory)\n",
        "            resume: Auto-resume from last checkpoint\n",
        "            patience: Early stopping patience\n",
        "            save_period: Checkpoint frequency (epochs)\n",
        "            lr0: Initial learning rate\n",
        "            optimizer: Optimizer (Adam, SGD, AdamW)\n",
        "            augment: Enable data augmentation\n",
        "            device: GPU device (0) or 'cpu'\n",
        "        \"\"\"\n",
        "\n",
        "        # Check for existing checkpoint\n",
        "        checkpoint_path = None\n",
        "        if resume:\n",
        "            checkpoint_path = self.find_latest_checkpoint()\n",
        "            if checkpoint_path:\n",
        "                self.backup_checkpoint(checkpoint_path)\n",
        "\n",
        "        # Initialize or resume model\n",
        "        if checkpoint_path and resume:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"RESUMING TRAINING FROM CHECKPOINT\")\n",
        "            print(f\"{'='*60}\\n\")\n",
        "            self.model = YOLO(checkpoint_path)\n",
        "        else:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"STARTING NEW TRAINING SESSION\")\n",
        "            print(f\"Model: YOLOv11{self.model_size}-OBB\")\n",
        "            print(f\"{'='*60}\\n\")\n",
        "            self.model = YOLO(f'yolo11{self.model_size}-obb.pt')\n",
        "\n",
        "        # Training configuration\n",
        "        train_config = {\n",
        "            'data': self.data_yaml,\n",
        "            'epochs': epochs,\n",
        "            'imgsz': imgsz,\n",
        "            'batch': batch,\n",
        "            'device': device,\n",
        "            'project': self.results_dir,\n",
        "            'name': 'train',\n",
        "            'exist_ok': True,\n",
        "            'patience': patience,\n",
        "            'save': True,\n",
        "            'save_period': save_period,\n",
        "            'cache': False,  # Don't cache to save disk space\n",
        "            'verbose': True,\n",
        "            'plots': True,\n",
        "            'val': True,\n",
        "            'resume': resume and (checkpoint_path is not None),\n",
        "            'lr0': lr0,\n",
        "            'optimizer': optimizer,\n",
        "            'augment': augment,\n",
        "            # Small object optimization\n",
        "            'mosaic': 1.0,      # Mosaic augmentation\n",
        "            'mixup': 0.1,       # Mixup augmentation\n",
        "            'copy_paste': 0.1,  # Copy-paste augmentation\n",
        "        }\n",
        "\n",
        "        # Merge additional arguments\n",
        "        train_config.update(kwargs)\n",
        "\n",
        "        # Print configuration\n",
        "        print(\"Training Configuration:\")\n",
        "        print(\"-\" * 60)\n",
        "        for key, value in train_config.items():\n",
        "            print(f\"  {key:20s}: {value}\")\n",
        "        print(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "        try:\n",
        "            # Start/resume training\n",
        "            results = self.model.train(**train_config)\n",
        "\n",
        "            # Save final model\n",
        "            final_name = f\"final_{self.model_size}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\"\n",
        "            final_path = os.path.join(self.checkpoint_dir, final_name)\n",
        "            self.model.save(final_path)\n",
        "            print(f\"\\nFinal model saved: {final_name}\")\n",
        "\n",
        "            return results\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nTraining interrupted! Progress saved.\")\n",
        "            print(\"Resume by re-running with resume=True\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(\"\\n GPU Out of Memory!\")\n",
        "                print(\"Solutions:\")\n",
        "                print(\"  1. Reduce batch size: batch=4 or batch=2\")\n",
        "                print(\"  2. Reduce image size: imgsz=640\")\n",
        "                print(\"  3. Use smaller model: model_size='n'\")\n",
        "                torch.cuda.empty_cache()\n",
        "            raise\n",
        "\n",
        "    def validate(self, checkpoint=None, split='val'):\n",
        "        \"\"\"Validate model on test/val set\"\"\"\n",
        "        if checkpoint is None:\n",
        "            checkpoint = self.find_latest_checkpoint()\n",
        "\n",
        "        if checkpoint is None:\n",
        "            print(\" No checkpoint found for validation\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"VALIDATION\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Checkpoint: {os.path.basename(checkpoint)}\")\n",
        "        print(f\"Split: {split}\\n\")\n",
        "\n",
        "        model = YOLO(checkpoint)\n",
        "        metrics = model.val(data=self.data_yaml, split=split)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"VALIDATION RESULTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"mAP50:    {metrics.box.map50:.4f}\")\n",
        "        print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
        "        print(f\"Precision: {metrics.box.mp:.4f}\")\n",
        "        print(f\"Recall:    {metrics.box.mr:.4f}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def predict(self, source, checkpoint=None, conf=0.25, iou=0.7, save=True):\n",
        "        \"\"\"Run inference on images/video\"\"\"\n",
        "        if checkpoint is None:\n",
        "            checkpoint = self.find_latest_checkpoint()\n",
        "\n",
        "        if checkpoint is None:\n",
        "            print(\" No checkpoint found for prediction\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\nRunning inference with {os.path.basename(checkpoint)}\")\n",
        "        model = YOLO(checkpoint)\n",
        "\n",
        "        results = model.predict(\n",
        "            source=source,\n",
        "            conf=conf,\n",
        "            iou=iou,\n",
        "            save=save,\n",
        "            project=os.path.join(self.project_root, 'predictions'),\n",
        "            name=f'predict_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "        )\n",
        "\n",
        "        return results\n",
        "\n",
        "    def export_model(self, format='onnx', checkpoint=None):\n",
        "        \"\"\"Export model to different formats\"\"\"\n",
        "        if checkpoint is None:\n",
        "            checkpoint = self.find_latest_checkpoint()\n",
        "\n",
        "        model = YOLO(checkpoint)\n",
        "        export_path = model.export(format=format)\n",
        "        print(f\" Model exported to {format}: {export_path}\")\n",
        "        return export_path\n"
      ],
      "metadata": {
        "id": "cWC0GSbD_dIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TRAINING EXECUTION\n",
        "\n",
        "\n",
        "# Initialize detector\n",
        "detector = SatelliteObjectDetector(\n",
        "    model_size='n',\n",
        "    data_yaml='DOTAv1.yaml',  # ← New dataset\n",
        "    project_root='/content/drive/MyDrive/soda_detection'\n",
        ")\n",
        "\n",
        "# Start training (automatically resumes if checkpoint exists)\n",
        "results = detector.train(\n",
        "    epochs=50,  # Fewer epochs for fine-tuning\n",
        "    imgsz=1024,\n",
        "    batch=8,\n",
        "    resume=False,  # Start fresh on new dataset\n",
        "    lr0=0.001,         # Learning rate\n",
        "    optimizer='Adam', # Adam, SGD, or AdamW\n",
        "    device=0,         # Use GPU\n",
        "    augment=True      # Enable augmentation for small objects\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Training complete!\")\n"
      ],
      "metadata": {
        "id": "BER6NtiJ_glO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MONITOR TRAINING WITH TENSORBOARD\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/satellite_detection/results/train\n",
        "\n",
        "\n",
        "# VISUALIZE TRAINING RESULTS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "results_path = '/content/drive/MyDrive/satellite_detection/results/train'\n",
        "\n",
        "# Display training plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "plots = ['results.png', 'confusion_matrix.png', 'F1_curve.png', 'PR_curve.png']\n",
        "\n",
        "for idx, plot_name in enumerate(plots):\n",
        "    plot_path = os.path.join(results_path, plot_name)\n",
        "    if os.path.exists(plot_path):\n",
        "        img = Image.open(plot_path)\n",
        "        axes[idx//2, idx%2].imshow(img)\n",
        "        axes[idx//2, idx%2].axis('off')\n",
        "        axes[idx//2, idx%2].set_title(plot_name.replace('.png', '').replace('_', ' ').title())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2EuLikJh_iIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# VALIDATE TRAINED MODEL\n",
        "\n",
        "\n",
        "# Validate on validation set\n",
        "val_metrics = detector.validate(split='val')\n",
        "\n",
        "# Validate on test set\n",
        "test_metrics = detector.validate(split='test')\n",
        "\n",
        "# Per-class performance\n",
        "print(\"\\nPer-Class Performance:\")\n",
        "print(\"-\" * 60)\n",
        "for i, class_name in enumerate(DOTA_CLASSES):\n",
        "    map50 = val_metrics.box.maps[i] if i < len(val_metrics.box.maps) else 0\n",
        "    print(f\"  {class_name:25s}: mAP50 = {map50:.4f}\")\n"
      ],
      "metadata": {
        "id": "vx8p9wwL_jzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# EXPORT TRAINED MODEL\n",
        "\n",
        "\n",
        "# Export to ONNX for deployment\n",
        "detector.export_model(format='onnx')\n",
        "\n",
        "# Export to TensorRT (GPU inference)\n",
        "detector.export_model(format='engine')\n",
        "\n",
        "# Export to TFLite (mobile deployment)\n",
        "detector.export_model(format='tflite')\n",
        "\n",
        "print(\"\\n Models exported successfully!\")\n"
      ],
      "metadata": {
        "id": "trOROCec_nNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}