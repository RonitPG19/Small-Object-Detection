{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2UEsd46nCQ",
        "outputId": "0b248314-39a3-46e3-a232-94acba7c6aaa"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# dotadevkit is the official package for DOTA data preparation\n",
        "!pip install -q dotadevkit\n",
        "!pip install -q opencv-python-headless\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.v2 as T\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import shutil # For file operations\n",
        "from google.colab import drive # For mounting Drive\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "\n",
        "# Set device\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNwMuYsU66yO",
        "outputId": "00fd366a-5be5-4e55-fbb3-947f94f2a056"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "folder_url = 'https://drive.google.com/drive/folders/1gmeE3D7R62UAtuIFOB9j2M5cUPTwtsxK?usp=drive_link'\n",
        "\n",
        "# Download the folder first to see structure\n",
        "gdown.download_folder(folder_url, output='/content/dataset', quiet=False)\n",
        "\n",
        "# Then unzip the image files\n",
        "for file in ['part1.zip', 'part2.zip', 'part3.zip']:\n",
        "    zip_path = f'/content/dataset/images/{file}'\n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"Unzipping {file}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/DOTA/')\n",
        "\n",
        "# Unzip label files\n",
        "labels_path = '/content/dataset/labelTxt-v1.0'\n",
        "if os.path.exists(labels_path):\n",
        "    for file in os.listdir(labels_path):\n",
        "        if file.endswith('.zip'):\n",
        "            print(f\"Unzipping {file}...\")\n",
        "            with zipfile.ZipFile(f'{labels_path}/{file}', 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content/DOTA/labelTxt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ny_19w--Kc"
      },
      "outputs": [],
      "source": [
        "# This is the formatted dir the devkit expects\n",
        "SOURCE_DATA_PATH = '/content/DOTA'\n",
        "# This is where the sliced patches will be saved\n",
        "PATCHES_PATH = '/content/DOTA_patches'\n",
        "os.makedirs(f'{PATCHES_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2GUAswzBBmI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmNyPD2D-oQb",
        "outputId": "05f37ed6-7d8e-4594-f158-02997211d8b8"
      },
      "outputs": [],
      "source": [
        "print(\"Starting data slicing with dotadevkit...\")\n",
        "# This will take a long time (15-30+ minutes)\n",
        "# Command: dotadevkit split <input_dir> <output_dir> <num_processes> <tile_size> <overlap>\n",
        "!dotadevkit split \"{SOURCE_DATA_PATH}\" \"{PATCHES_PATH}\" 8 1024 200\n",
        "\n",
        "print(\"Slicing complete.\")\n",
        "print(f\"Patches saved to {PATCHES_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23nfHXgT_VxI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.v2 as T\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np # Make sure numpy is imported\n",
        "\n",
        "# Define data augmentation transforms\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToImage()) # Convert PIL image to tensor\n",
        "    transforms.append(T.ToDtype(torch.float32, scale=True)) # Normalize to [0, 1]\n",
        "    if train:\n",
        "        # Simple augmentation for training\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "class DOTAPatchDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads DOTA patch images and their 10-part OBB annotations.\n",
        "    - Filters for the class at parts[8]\n",
        "    - Converts 8 OBB coordinates to 4 HBB coordinates\n",
        "\n",
        "    *** VERBOSE DEBUG VERSION ***\n",
        "    \"\"\"\n",
        "    def __init__(self, patches_dir, transforms):\n",
        "        self.patches_dir = patches_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.img_dir = os.path.join(patches_dir, \"images\")\n",
        "        self.ann_dir = os.path.join(patches_dir, \"labelTxt\")\n",
        "\n",
        "        all_img_files = [f for f in os.listdir(self.img_dir) if f.endswith('.png')]\n",
        "\n",
        "        self.img_files = []\n",
        "        print(f\"--- STARTING VERBOSE FILTER ---\")\n",
        "        print(f\"Found {len(all_img_files)} total image patches.\")\n",
        "        print(f\"Now checking for 'small-vehicle' in corresponding annotation files...\")\n",
        "\n",
        "        # To avoid spamming, we'll only print first 10 successes and failures\n",
        "        success_prints = 0\n",
        "        fail_prints = 0\n",
        "\n",
        "        for img_name in all_img_files:\n",
        "            ann_name = img_name.replace('.png', '.txt')\n",
        "            ann_path = os.path.join(self.ann_dir, ann_name)\n",
        "\n",
        "            if not os.path.exists(ann_path):\n",
        "                print(f\"  [SKIP] Image {img_name} has no matching annotation file.\")\n",
        "                continue\n",
        "\n",
        "            found = False\n",
        "            try:\n",
        "                with open(ann_path, 'r') as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        # Check for 10 parts (8 coords + class + difficulty)\n",
        "                        # and check parts[8] for the class name\n",
        "                        if len(parts) >= 9 and parts[8] == 'small-vehicle':\n",
        "                            found = True\n",
        "                            break # Found it!\n",
        "            except Exception as e:\n",
        "                print(f\"  [ERROR] Could not read {ann_name}. Error: {e}\")\n",
        "                continue\n",
        "\n",
        "            if found:\n",
        "                self.img_files.append(img_name)\n",
        "                if success_prints < 10:\n",
        "                     print(f\"  [SUCCESS] Found 'small-vehicle' in {ann_name}!\")\n",
        "                     success_prints += 1\n",
        "            else:\n",
        "                if fail_prints < 10:\n",
        "                     print(f\"  [FAIL] No 'small-vehicle' found in {ann_name}.\")\n",
        "                     fail_prints += 1\n",
        "\n",
        "        print(f\"--- VERBOSE FILTER COMPLETE ---\")\n",
        "        print(f\"Filter complete. Found {len(self.img_files)} images with 'small-vehicle'.\")\n",
        "        if len(self.img_files) == 0:\n",
        "            print(\"Warning: No images with 'small-vehicle' were found.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_name = self.img_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Load annotations\n",
        "        ann_name = img_name.replace('.png', '.txt')\n",
        "        ann_path = os.path.join(self.ann_dir, ann_name)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                # Check for 10 parts and class at parts[8]\n",
        "                if len(parts) >= 9 and parts[8] == 'small-vehicle':\n",
        "                    try:\n",
        "                        # 1. Get the 8 OBB coordinates\n",
        "                        obb_coords = np.array([float(p) for p in parts[:8]]).reshape(4, 2)\n",
        "\n",
        "                        # 2. Convert OBB to HBB (Horizontal Bounding Box)\n",
        "                        xmin = np.min(obb_coords[:, 0])\n",
        "                        ymin = np.min(obb_coords[:, 1])\n",
        "                        xmax = np.max(obb_coords[:, 0])\n",
        "                        ymax = np.max(obb_coords[:, 1])\n",
        "\n",
        "                        # 3. Add to list\n",
        "                        if xmax > xmin and ymax > ymin:\n",
        "                            boxes.append([xmin, ymin, xmax, ymax])\n",
        "                            labels.append(1) # '1' for small-vehicle\n",
        "                    except ValueError:\n",
        "                        print(f\"Skipping malformed line in {ann_name}: {line}\")\n",
        "                        continue\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros(0, dtype=torch.int64)\n",
        "            area = torch.zeros(0, dtype=torch.float32)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = torch.tensor([idx])\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "# Collate function for the DataLoader\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_YrrOE__YgM"
      },
      "outputs": [],
      "source": [
        "def get_model(num_classes=2):\n",
        "    # Load a pre-trained Faster R-CNN model\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # Get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSYfyJmT_do5",
        "outputId": "0ebecc4c-8615-428f-ab0b-33601460dc67"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.005\n",
        "# ---------------------\n",
        "\n",
        "# 1. Prepare DataLoaders\n",
        "full_dataset = DOTAPatchDataset(PATCHES_PATH, get_transform(train=True))\n",
        "\n",
        "# Split dataset into training and validation (90% train, 10% val)\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# We must apply the 'train' transform to the train_dataset\n",
        "# and the 'test' transform to the val_dataset.\n",
        "# We do this by \"re-wrapping\" them.\n",
        "train_dataset.dataset.transforms = get_transform(train=True)\n",
        "val_dataset.dataset.transforms = get_transform(train=False)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Training on {len(train_dataset)} images, Validating on {len(val_dataset)} images.\")\n",
        "\n",
        "# 2. Get Model\n",
        "model = get_model(num_classes=2)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# 3. Set up Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "print(\"--- Starting Training ---\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for i, (images, targets) in enumerate(train_loader):\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += losses.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {losses.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # Update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.train() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_loader:\n",
        "            images = list(image.to(DEVICE) for image in images)\n",
        "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # During eval, model still needs targets to calculate loss\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            total_val_loss += losses.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"--- Epoch {epoch+1} Summary ---\")\n",
        "    print(f\"Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Avg Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "# Save the trained model\n",
        "MODEL_SAVE_PATH = '/content/fasterrcnn_dota.pth'\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"Training complete. Model saved to {MODEL_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KcodTqkHxpp"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This is the path where you saved your model\n",
        "MODEL_SAVE_PATH = '/content/fasterrcnn_dota.pth'\n",
        "\n",
        "print(f\"Downloading {MODEL_SAVE_PATH}...\")\n",
        "\n",
        "# This command triggers the download\n",
        "files.download(MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mkrb3WCH9Mj"
      },
      "outputs": [],
      "source": [
        "# Helper function to visualize\n",
        "def visualize_prediction(img, pred, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Draws the image and predicted bounding boxes.\n",
        "    \"\"\"\n",
        "    img = img.cpu().permute(1, 2, 0).numpy() # Convert from (C, H, W) to (H, W, C)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    for box, score, label in zip(pred['boxes'], pred['scores'], pred['labels']):\n",
        "        if score > threshold:\n",
        "            # DOTA boxes are [xmin, ymin, xmax, ymax]\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "\n",
        "            # Create a Rectangle patch\n",
        "            rect = patches.Rectangle(\n",
        "                (xmin, ymin),\n",
        "                width,\n",
        "                height,\n",
        "                linewidth=2,\n",
        "                edgecolor='r',\n",
        "                facecolor='none'\n",
        "            )\n",
        "\n",
        "            # Add the patch to the Axes\n",
        "            ax.add_patch(rect)\n",
        "            plt.text(\n",
        "                xmin, ymin - 5,\n",
        "                f'Vehicle: {score:.2f}',\n",
        "                bbox=dict(facecolor='red', alpha=0.5, pad=0),\n",
        "                color='white'\n",
        "            )\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- Load Model and Run Inference ---\n",
        "\n",
        "# 1. Load the saved model\n",
        "model = get_model(num_classes=2)\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "model.to(DEVICE)\n",
        "model.eval() # Set to evaluation mode\n",
        "\n",
        "# 2. Get a random image from the validation set\n",
        "img, target = random.choice(val_dataset)\n",
        "\n",
        "# 3. Run inference\n",
        "with torch.no_grad():\n",
        "    # Add a batch dimension and send to device\n",
        "    prediction = model([img.to(DEVICE)])[0] # [0] to get first item in batch\n",
        "\n",
        "# 4. Visualize\n",
        "print(\"Displaying prediction on a random validation image:\")\n",
        "visualize_prediction(img, prediction)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
